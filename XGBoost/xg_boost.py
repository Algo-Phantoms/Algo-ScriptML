# -*- coding: utf-8 -*-
"""XG-BOOST.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PRue8_WnzGJ4rjj9RoGjzP9kgIUTAShq

Hi , here is the basic implementation of XGBoost using the Iris Dataset with some hyper parameter tuning
"""

# Importing the necessary libraries
import pandas as pd
import numpy as np
import seaborn as sns

#loading the Iris dataset using seaborn
df = sns.load_dataset('iris')

#mapping the target/label values
df['species']=df['species'].map({'setosa':0,'versicolor':1,'virginica':2})

#seperating the feature and label dataframes
X=df.drop(['species'],axis=1)
y=df['species']

"""building the model """

from sklearn.model_selection import train_test_split

X_train,X_test,y_train,y_test=train_test_split(X,y, test_size=0.2,random_state=0)

#importing XGBoost
import xgboost
from xgboost import XGBClassifier

clf = XGBClassifier()

#fitting the classifier
clf.fit(X_train,y_train)

#storing the predictions in y_pred
y_pred = clf.predict(X_test)

#importing metrices
from sklearn.metrics import  classification_report, accuracy_score, confusion_matrix

"""Results"""

accuracy_score(y_test,y_pred)

print(confusion_matrix(y_test,y_pred))

print(classification_report(y_test,y_pred))

"""Performing basic hyperparameter tuning 

"""

from sklearn.model_selection import GridSearchCV

param_grid = {
    'n_estimators': [40,50,100, 200, 300, 500],
    'learning_rate' : [0.0001, 0.001, 0.01, 0.1, 1.0],
}

xgclf = XGBClassifier()
grid_search = GridSearchCV(estimator = xgclf, param_grid = param_grid, 
                          cv = 3, n_jobs = -1, verbose = 2)

grid_search.fit(X_train,y_train)

#est parameters 
grid_search.best_params_

